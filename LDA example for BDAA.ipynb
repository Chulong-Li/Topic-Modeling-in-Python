{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2c19fa4570a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from pprint import pprint as pprint\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk.stem.wordnet\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cinci_data_bdaa.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-642fd423d55d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cinci_data_bdaa.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cinci_data_bdaa.json'"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "with open('cinci_data_bdaa.json') as f:\n",
    "    data = json.load(f)\n",
    "        \n",
    "print(timer() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c18c2bb1f1be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkey_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtemp_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mkey_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "key_set = set()\n",
    "\n",
    "for post in data: \n",
    "    temp_keys = post.keys()\n",
    "    key_set = key_set.union(temp_keys)\n",
    "    \n",
    "pprint(key_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-08c42af1ea97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtype_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpost_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtype_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpost_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#make sure to include the brackets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "type_set = set()\n",
    "\n",
    "for post in data: \n",
    "    post_type = post['type']\n",
    "    type_set = type_set.union([post_type]) #make sure to include the brackets\n",
    "    \n",
    "pprint(type_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pare down the size of the data \n",
    "test_data = [data[i] for i in random.sample(xrange(len(data)), 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only the relevant information \n",
    "\n",
    "documents = []\n",
    "for post in test_data: \n",
    "    str_list = []\n",
    "    if post.has_key('caption'): \n",
    "        str_list.append(post['caption'])\n",
    "    if post.has_key('lname'): \n",
    "        str_list.append(post['lname'])\n",
    "    if post.has_key('description'): \n",
    "        str_list.append(post['description'])\n",
    "    if post.has_key('types'): \n",
    "        str_list.append(str(post['types'])) #don't forget to include str here! \n",
    "    documents.append(' '.join(str_list))\n",
    "    \n",
    "pprint(documents[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "cleaned_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc = doc.lower() #lowercase\n",
    "    doc = re.sub(\"http(.*?) \",' ',doc) #remove almost all links\n",
    "    doc = re.sub(\"http(.*)\",' ',doc) #remove links that came at the end of a doc\n",
    "    doc = re.sub(\"u'\",'',doc) #remove all the unicode identifiers\n",
    "    doc = re.sub(r'[{}]'.format(string.punctuation),\" \",doc) #remove punctuation\n",
    "    doc = re.sub('[â€¢\\t\\n\\r\\f\\v]', ' ', doc) #remove newline characters\n",
    "    doc = re.sub(\"  +\",\" \",doc) #remove multiple spaces\n",
    "    doc = re.sub(\" [a-z0-9] \",\"\",doc) #remove single letter/digit words\n",
    "    \n",
    "    tokens = tokenizer.tokenize(doc) \n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens] \n",
    "    non_numeric_tokens = [token for token in lemmatized_tokens if not token.isdigit()] #remove just numbers\n",
    "    longer_than_1_tokens = [token for token in non_numeric_tokens if len(token) > 1] #docs must have >1 word\n",
    "    cleaned_documents.append(longer_than_1_tokens)\n",
    "\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(cleaned_documents)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in cleaned_documents]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 5000\n",
    "passes_n = 20\n",
    "num_topics = 20\n",
    "\n",
    "start = timer()\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=passes_n,\n",
    "                                           chunksize = chunksize, alpha = 'auto', eta = 'auto', )\n",
    "\n",
    "print(\"time to finish model with {} topics: {}\".format(i, timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_topics = ldamodel.top_topics(corpus)\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words_to_file2(top_topics, n_words, text_file): \n",
    "    i = 0\n",
    "    for topic in top_topics: \n",
    "        message = \"topic #{}: \".format(i)\n",
    "        i += 1\n",
    "        j = 0\n",
    "        for word in topic[0]: \n",
    "            message += str(word[1]) + \" \"\n",
    "            j+= 1\n",
    "            if j > n_words:\n",
    "                break;\n",
    "        text_file.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('bdaa_LDA_out_text', 'w') as f: \n",
    "    top_topics = ldamodel.top_topics(corpus)\n",
    "    pprint(top_topics)\n",
    "    print_top_words_to_file2(top_topics, 15, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
