{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from pprint import pprint as pprint\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk.stem.wordnet\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "with open('cinci_data_bdaa.json') as f:\n",
    "    data = json.load(f)\n",
    "        \n",
    "print(timer() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_set = set()\n",
    "\n",
    "for post in data: \n",
    "    temp_keys = post.keys()\n",
    "    key_set = key_set.union(temp_keys)\n",
    "    \n",
    "pprint(key_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_set = set()\n",
    "\n",
    "for post in data: \n",
    "    post_type = post['type']\n",
    "    type_set = type_set.union([post_type]) #make sure to include the brackets\n",
    "    \n",
    "pprint(type_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pare down the size of the data \n",
    "test_data = [data[i] for i in random.sample(xrange(len(data)), 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select only the relevant information \n",
    "\n",
    "documents = []\n",
    "for post in test_data: \n",
    "    str_list = []\n",
    "    if post.has_key('caption'): \n",
    "        str_list.append(post['caption'])\n",
    "    if post.has_key('lname'): \n",
    "        str_list.append(post['lname'])\n",
    "    if post.has_key('description'): \n",
    "        str_list.append(post['description'])\n",
    "    if post.has_key('types'): \n",
    "        str_list.append(str(post['types'])) #don't forget to include str here! \n",
    "    documents.append(' '.join(str_list))\n",
    "    \n",
    "pprint(documents[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "cleaned_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc = doc.lower() #lowercase\n",
    "    doc = re.sub(\"http(.*?) \",' ',doc) #remove almost all links\n",
    "    doc = re.sub(\"http(.*)\",' ',doc) #remove links that came at the end of a doc\n",
    "    doc = re.sub(\"u'\",'',doc) #remove all the unicode identifiers\n",
    "    doc = re.sub(r'[{}]'.format(string.punctuation),\" \",doc) #remove punctuation\n",
    "    doc = re.sub('[â€¢\\t\\n\\r\\f\\v]', ' ', doc) #remove newline characters\n",
    "    doc = re.sub(\"  +\",\" \",doc) #remove multiple spaces\n",
    "    doc = re.sub(\" [a-z0-9] \",\"\",doc) #remove single letter/digit words\n",
    "    \n",
    "    tokens = tokenizer.tokenize(doc) \n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens] \n",
    "    non_numeric_tokens = [token for token in lemmatized_tokens if not token.isdigit()] #remove just numbers\n",
    "    longer_than_1_tokens = [token for token in non_numeric_tokens if len(token) > 1] #docs must have >1 word\n",
    "    cleaned_documents.append(longer_than_1_tokens)\n",
    "\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(cleaned_documents)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in cleaned_documents]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunksize = 5000\n",
    "passes_n = 20\n",
    "num_topics = 20\n",
    "\n",
    "start = timer()\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=passes_n,\n",
    "                                           chunksize = chunksize, alpha = 'auto', eta = 'auto', )\n",
    "\n",
    "print(\"time to finish model with {} topics: {}\".format(i, timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_topics = ldamodel.top_topics(corpus)\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words_to_file2(top_topics, n_words, text_file): \n",
    "    i = 0\n",
    "    for topic in top_topics: \n",
    "        message = \"topic #{}: \".format(i)\n",
    "        i += 1\n",
    "        j = 0\n",
    "        for word in topic[0]: \n",
    "            message += str(word[1]) + \" \"\n",
    "            j+= 1\n",
    "            if j > n_words:\n",
    "                break;\n",
    "        text_file.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('bdaa_LDA_out_text', 'w') as f: \n",
    "    top_topics = ldamodel.top_topics(corpus)\n",
    "    pprint(top_topics)\n",
    "    print_top_words_to_file2(top_topics, 15, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
